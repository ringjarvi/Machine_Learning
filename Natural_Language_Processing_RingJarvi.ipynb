{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "JmevfBArmSyR",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "019fc99a17a5172c9536f14e715a3d53",
     "grade": false,
     "grade_id": "cell-0d4717fe7700aa92",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "In this exercise we will calculate a variety of feature extraction methods on a news article dataset and use various classifiers to predict the article's category.\n",
    "\n",
    "We will first use classical methods for feature extraction with Naive Bayes, followed by more recent methods of using word embeddings with a simple Linear SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "UIOYV911mSyT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad29b177dd5a0376cd85ba9e39209fec",
     "grade": false,
     "grade_id": "cell-7ff8992190e52386",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "bcec8aa8-ad55-4cc0-fd1d-e12995ca1836"
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-0c38443f5617>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-0c38443f5617>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    get_ipython().system(' pip install spacy')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# # Uncomment the below line to install\n",
    " ! pip install spacy\n",
    " ! python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/rrj/anaconda3/lib/python3.7/site-packages (2.2.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.0 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy) (7.4.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy) (1.18.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: setuptools in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy) (44.0.0.post20200106)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy) (4.41.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/rrj/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /Users/rrj/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (8.0.2)\n",
      "Requirement already satisfied: en_core_web_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz#egg=en_core_web_md==2.2.5 in /Users/rrj/anaconda3/lib/python3.7/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: setuptools in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (44.0.0.post20200106)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.18.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.25.7)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/rrj/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/rrj/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /Users/rrj/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (8.0.2)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy\n",
    "! python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "9ijrZtzTmSyW",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56a4068f8a3db8d2189569cde657aa44",
     "grade": false,
     "grade_id": "cell-839fb7e9db39b4dd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "B6HDUcKGmSyZ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50df3022d50dc10fd18cd71acf03c2f5",
     "grade": false,
     "grade_id": "cell-32547bf8b3c17d81",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(subset=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(subset=\"all\", remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "h5oh1F04mSyb",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ee06fb20e81b0a6aeece342974b39bb",
     "grade": false,
     "grade_id": "cell-222fb692deae0d18",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "02d81803-54fe-48b2-9f9a-d87e2cf2ec57",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n",
      "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "fetching / caching functions that downloads the data archive from\n",
      "the original `20 newsgroups website`_, extracts the archive contents\n",
      "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      ":func:`sklearn.datasets.load_files` on either the training or\n",
      "testing set folder, or both of them::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_20newsgroups\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "  >>> from pprint import pprint\n",
      "  >>> pprint(list(newsgroups_train.target_names))\n",
      "  ['alt.atheism',\n",
      "   'comp.graphics',\n",
      "   'comp.os.ms-windows.misc',\n",
      "   'comp.sys.ibm.pc.hardware',\n",
      "   'comp.sys.mac.hardware',\n",
      "   'comp.windows.x',\n",
      "   'misc.forsale',\n",
      "   'rec.autos',\n",
      "   'rec.motorcycles',\n",
      "   'rec.sport.baseball',\n",
      "   'rec.sport.hockey',\n",
      "   'sci.crypt',\n",
      "   'sci.electronics',\n",
      "   'sci.med',\n",
      "   'sci.space',\n",
      "   'soc.religion.christian',\n",
      "   'talk.politics.guns',\n",
      "   'talk.politics.mideast',\n",
      "   'talk.politics.misc',\n",
      "   'talk.religion.misc']\n",
      "\n",
      "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "attribute is the integer index of the category::\n",
      "\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "It is possible to load only a sub-selection of the categories by passing the\n",
      "list of the categories to load to the\n",
      ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "  >>> cats = ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "  >>> list(newsgroups_train.target_names)\n",
      "  ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      "Converting text to vectors\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "In order to feed predictive or clustering models with the text data,\n",
      "one first need to turn the text into vectors of numerical values suitable\n",
      "for statistical analysis. This can be achieved with the utilities of the\n",
      "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "example that extract `TF-IDF`_ vectors of unigram tokens\n",
      "from a subset of 20news::\n",
      "\n",
      "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "  ...               'comp.graphics', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectorizer = TfidfVectorizer()\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> vectors.shape\n",
      "  (2034, 34118)\n",
      "\n",
      "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "components by sample in a more than 30000-dimensional space\n",
      "(less than .5% non-zero features)::\n",
      "\n",
      "  >>> vectors.nnz / float(vectors.shape[0])\n",
      "  159.01327...\n",
      "\n",
      ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \n",
      "returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
      ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
      "\n",
      "\n",
      "Filtering text for more realistic training\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "It is easy for a classifier to overfit on particular things that appear in the\n",
      "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "high F-scores, but their results would not generalize to other documents that\n",
      "aren't from this window of time.\n",
      "\n",
      "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "which is fast to train and achieves a decent F-score::\n",
      "\n",
      "  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "  >>> from sklearn import metrics\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.88213...\n",
      "\n",
      "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "the training and test data, instead of segmenting by time, and in that case\n",
      "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "yet of what's going on inside this classifier?)\n",
      "\n",
      "Let's take a look at what the most informative features are:\n",
      "\n",
      "  >>> import numpy as np\n",
      "  >>> def show_top10(classifier, vectorizer, categories):\n",
      "  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n",
      "  ...     for i, category in enumerate(categories):\n",
      "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "  ...\n",
      "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "  alt.atheism: edu it and in you that is of to the\n",
      "  comp.graphics: edu in graphics it is for and of to the\n",
      "  sci.space: edu it that is in and space to of the\n",
      "  talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "You can now see many things that these features have overfit to:\n",
      "\n",
      "- Almost every group is distinguished by whether headers such as\n",
      "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "- Another significant feature involves whether the sender is affiliated with\n",
      "  a university, as indicated either by their headers or their signature.\n",
      "- The word \"article\" is a significant feature, based on how often people quote\n",
      "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "  wrote:\"\n",
      "- Other features match the names and e-mail addresses of particular people who\n",
      "  were posting at the time.\n",
      "\n",
      "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "barely have to identify topics from text at all, and they all perform at the\n",
      "same high level.\n",
      "\n",
      "For this reason, the functions that load 20 Newsgroups data provide a\n",
      "parameter called **remove**, telling it what kinds of information to strip out\n",
      "of each file. **remove** should be a tuple containing any subset of\n",
      "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "blocks, and quotation blocks respectively.\n",
      "\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
      "  0.77310...\n",
      "\n",
      "This classifier lost over a lot of its F-score, just because we removed\n",
      "metadata that has little to do with topic classification.\n",
      "It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.76995...\n",
      "\n",
      "Some other classifiers cope better with this harder version of the task. Try\n",
      "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
      "the ``--filter`` option to compare the results.\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n",
      "  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "  lower because it is more realistic.\n",
      "\n",
      ".. topic:: Examples\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "rzVJ_K5DmSyd",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48fa44e3ed13130dd58386fbc41f1801",
     "grade": false,
     "grade_id": "cell-c1f44b15459bea17",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "3852899f-a79b-4329-a601-95832b80f38f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are the 20 topics that an article can belong to:\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "text = data[\"data\"]\n",
    "target = data[\"target\"]\n",
    "print(\"The following are the 20 topics that an article can belong to:\")\n",
    "print(data[\"target_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "iT1TVBPnmSyf",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2d0239fa14b9e082a02968d7c142595",
     "grade": false,
     "grade_id": "cell-3792303ddcf70dbb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text, target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "auAVG_yLmSyh",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac68ffd4d73dff7c10eea2e716b9040a",
     "grade": false,
     "grade_id": "cell-2377d43a03f72b0f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "5af75f50-212e-449a-8c6f-5ec9ed1e9663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset contains 14134 articles.\n",
      "The test dataset contains 4712 articles.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The training dataset contains {len(X_train)} articles.\")\n",
    "print(f\"The test dataset contains {len(X_test)} articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "i68ja0VPmSyj",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1991f8c56f04db0dad84fea9274d2f5",
     "grade": false,
     "grade_id": "cell-b2b050fc5a1f96db",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Scikit learn implements the BoW feature representation using [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), and it also has implementations for [TF-IDF](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) and [hashed vector](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer) representations.\n",
    "\n",
    "Determine the feature representations of our dataset using each of those approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "deletable": false,
    "id": "gatpylAbmSyk",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45ef423d08f71eebb73f9e0f431566d2",
     "grade": false,
     "grade_id": "cell-f899f3942f01be93",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "0d2f29e0-698c-4911-b692-7f8acd37e811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.3 s, sys: 407 ms, total: 16.7 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use English stopwords and produce a BoW representation for the data using up to trigrams\n",
    "# Save the vectorizer as counter and the transformed data as X_train_bow, and X_test_bow\n",
    "# YOUR CODE HERE\n",
    "counter = CountVectorizer(stop_words='english',ngram_range=(1, 3)).fit(X_train, y_train)\n",
    "X_train_bow = counter.transform(X_train)\n",
    "X_test_bow=counter.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2262337\n",
      "(14134, 2262337)\n",
      "(4712, 2262337)\n"
     ]
    }
   ],
   "source": [
    "print(len(counter.get_feature_names()))\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "D43STXj5mSym",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54e331ac2a899a8d165562540b4e9c49",
     "grade": true,
     "grade_id": "cell-740433b073b13510",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-33832fc23a1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"english\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngram_range\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3034327\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mX_train_bow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m14134\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3034327\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mX_test_bow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4712\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3034327\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert counter\n",
    "assert counter.stop_words == \"english\"\n",
    "assert counter.ngram_range == (1,3)\n",
    "assert len(counter.get_feature_names()) == 3034327\n",
    "assert X_train_bow.shape == (14134, 3034327)\n",
    "assert X_test_bow.shape == (4712, 3034327)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "4CmYjNVemSyo",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "630143b0c7f771b9c5855dbb5caf411d",
     "grade": false,
     "grade_id": "cell-cd1cb3b0c71eedd9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Note that sklearn implements a [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html). The main difference between the two is in the inputs to fitting and transforming. The [Vectorizer's fit/transform](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.fit) take an input of text whereas the [transformer's](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer.fit) take an input of a BoW vector. Given that we already determined the BoW vectors, it would be more time efficient to use TfidfTransformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "deletable": false,
    "id": "qRUqX4LGmSyo",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dee5b8fbe12f0499a87727cd864d345c",
     "grade": false,
     "grade_id": "cell-018bdbbae2613aef",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "9918d54e-a7b7-4aa2-84ef-9c7b03e4c1d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.02 s, sys: 76.9 ms, total: 1.1 s\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use the BoW representation you just created above to produce a TFIDF representation of the data\n",
    "# Save the transformer to tfidfer and the transformed data as X_train_tfidf, and X_test_tfidf\n",
    "\n",
    "# YOUR CODE HERE\n",
    "tfidfer=TfidfTransformer()\n",
    "X_train_tfidf=tfidfer.fit_transform(X_train_bow)\n",
    "X_test_tfidf=tfidfer.transform(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14134, 2262337)\n",
      "(4712, 2262337)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "uowNUef5mSyq",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ecdb4782c18f141c55bfeefc8dec8c48",
     "grade": true,
     "grade_id": "cell-0e6ce5999cd93389",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-28744c3ed56d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtfidfer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m  \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m14134\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3034327\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mX_test_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m  \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4712\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3034327\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert tfidfer\n",
    "assert X_train_tfidf.shape  == (14134, 3034327)\n",
    "assert X_test_tfidf.shape  == (4712, 3034327)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "qjUcn6tXmSys",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f5c54e66ea2ef754e096d6523435c69",
     "grade": false,
     "grade_id": "cell-ed956dd2a828dea8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now use the hashing vectorizer to do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "deletable": false,
    "id": "uajEdezpmSyt",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7dcaa4ad0f713ed4155a48876ec8b41b",
     "grade": false,
     "grade_id": "cell-2ad00d9b9df56ed5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "33a1471b-9d66-4a04-c181-88677124e0cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.5 s, sys: 55.1 ms, total: 3.56 s\n",
      "Wall time: 3.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Use English stopwords and produce a Hashed vector representation for the data using up to trigrams\n",
    "# Save the vectorizer as hasher and the transformed data as X_train_hash, and X_test_hash\n",
    "# Make sure you set alternate_sign to False so we can use this representation with Multinomial Naive Bayes\n",
    "\n",
    "# YOUR CODE HERE\n",
    "hasher = HashingVectorizer(stop_words='english', ngram_range=(1,3), alternate_sign=False)\n",
    "X_train_hash = hasher.fit_transform(X_train)\n",
    "X_test_hash = hasher.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "ceNTTZK9mSyv",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fee5c79f5236e12894fdf52279d3f9a",
     "grade": true,
     "grade_id": "cell-f6a5afb5d4f0a4d9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert hasher\n",
    "assert hasher.stop_words == \"english\"\n",
    "assert hasher.ngram_range == (1,3)\n",
    "assert X_train_hash.shape == (14134, 1048576)\n",
    "assert X_test_hash.shape == (4712, 1048576)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "tSbrqTJAmSyx",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c7c128b56ddb4e69ecac2078a7fa5a0",
     "grade": false,
     "grade_id": "cell-f81192642c9b9950",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Compare the time it took to run the count vectorizer vs the hashing vectorizer even though they both will iterate through all the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "BcTbaGQomSyx",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e5d5d7774aecfc0c87da6a25ef08d25d",
     "grade": false,
     "grade_id": "cell-279d25a3bcd86cf6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now recall [Naive Bayes Classification](http://scikit-learn.org/stable/modules/naive_bayes.html) which we discussed early on in the supervised learning lectures. We will use Naive Bayes classifiers to predict the topic of the articles and compare our feature representations. Use a Multinomial Naive Bayes classifier to predict the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "deletable": false,
    "id": "sW0ZcWDymSyy",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5257b0ce184bcaec3437d5f17b374a3",
     "grade": false,
     "grade_id": "cell-cdf5ba26c387fd97",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "a467eaa1-9184-4761-a785-193db8d943f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Bag of Words\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.34      0.47       205\n",
      "           1       0.56      0.74      0.64       245\n",
      "           2       0.93      0.42      0.57       250\n",
      "           3       0.61      0.74      0.67       243\n",
      "           4       0.88      0.63      0.73       255\n",
      "           5       0.64      0.88      0.74       240\n",
      "           6       0.85      0.69      0.76       249\n",
      "           7       0.50      0.73      0.59       219\n",
      "           8       0.96      0.52      0.68       246\n",
      "           9       0.96      0.77      0.85       227\n",
      "          10       0.90      0.83      0.86       287\n",
      "          11       0.55      0.85      0.67       234\n",
      "          12       0.88      0.47      0.61       247\n",
      "          13       0.84      0.86      0.85       250\n",
      "          14       0.80      0.80      0.80       240\n",
      "          15       0.50      0.90      0.64       250\n",
      "          16       0.77      0.61      0.68       211\n",
      "          17       0.49      0.89      0.64       246\n",
      "          18       0.65      0.58      0.61       209\n",
      "          19       0.85      0.22      0.35       159\n",
      "\n",
      "    accuracy                           0.69      4712\n",
      "   macro avg       0.75      0.67      0.67      4712\n",
      "weighted avg       0.75      0.69      0.68      4712\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Results for TF-IDF\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.25      0.39       205\n",
      "           1       0.69      0.69      0.69       245\n",
      "           2       0.73      0.65      0.69       250\n",
      "           3       0.60      0.79      0.68       243\n",
      "           4       0.87      0.64      0.74       255\n",
      "           5       0.79      0.85      0.82       240\n",
      "           6       0.82      0.80      0.81       249\n",
      "           7       0.49      0.83      0.62       219\n",
      "           8       0.90      0.72      0.80       246\n",
      "           9       0.81      0.85      0.83       227\n",
      "          10       0.93      0.86      0.89       287\n",
      "          11       0.65      0.85      0.74       234\n",
      "          12       0.84      0.62      0.71       247\n",
      "          13       0.89      0.84      0.86       250\n",
      "          14       0.84      0.82      0.83       240\n",
      "          15       0.41      0.94      0.57       250\n",
      "          16       0.68      0.77      0.73       211\n",
      "          17       0.78      0.84      0.81       246\n",
      "          18       0.95      0.34      0.50       209\n",
      "          19       0.67      0.03      0.05       159\n",
      "\n",
      "    accuracy                           0.72      4712\n",
      "   macro avg       0.76      0.70      0.69      4712\n",
      "weighted avg       0.76      0.72      0.70      4712\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Results for Hashing\n",
      "--------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.16      0.27       205\n",
      "           1       0.64      0.65      0.64       245\n",
      "           2       0.71      0.60      0.65       250\n",
      "           3       0.55      0.78      0.64       243\n",
      "           4       0.88      0.59      0.71       255\n",
      "           5       0.76      0.82      0.79       240\n",
      "           6       0.80      0.76      0.78       249\n",
      "           7       0.48      0.80      0.60       219\n",
      "           8       0.89      0.65      0.75       246\n",
      "           9       0.78      0.83      0.81       227\n",
      "          10       0.93      0.83      0.88       287\n",
      "          11       0.57      0.85      0.68       234\n",
      "          12       0.83      0.57      0.68       247\n",
      "          13       0.88      0.79      0.83       250\n",
      "          14       0.83      0.80      0.81       240\n",
      "          15       0.36      0.94      0.52       250\n",
      "          16       0.65      0.72      0.68       211\n",
      "          17       0.80      0.81      0.80       246\n",
      "          18       0.94      0.22      0.35       209\n",
      "          19       0.33      0.01      0.01       159\n",
      "\n",
      "    accuracy                           0.68      4712\n",
      "   macro avg       0.73      0.66      0.64      4712\n",
      "weighted avg       0.73      0.68      0.66      4712\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for feat_name, train_feat, test_feat in zip([\"Bag of Words\", \"TF-IDF\", \"Hashing\"],[X_train_bow, X_train_tfidf, X_train_hash], [X_test_bow, X_test_tfidf, X_test_hash]):\n",
    "    # Create a Multinomial Naive Bayes model saved to `mnb` and fit it to train_feat\n",
    "    # YOUR CODE HERE\n",
    "  \n",
    "    mnb = MultinomialNB().fit(train_feat, y_train)\n",
    "    \n",
    "\n",
    "\n",
    "    y_pred = mnb.predict(test_feat)\n",
    "    print(f\"Results for {feat_name}\")\n",
    "    print(\"-\"*80)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "aBU7ZnwvmSy2",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6868948856264f42927eef81e550203",
     "grade": true,
     "grade_id": "cell-b545c8d3793a05e7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(mnb, MultinomialNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "E3Gb0Y7MmSy4",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e12e32a8bcd208a60db88b25a2f2ab2",
     "grade": false,
     "grade_id": "cell-f1fb206ff47bb7a6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Learned Embeddings\n",
    "\n",
    "We will use [spacy](https://spacy.io/) for more sophisticated NLP. Make sure you downloaded the english model in the commented code at the top of the notebook before proceeding. It may take some time to download.\n",
    "\n",
    "Spacy allows us to parse text and automatically does the following:\n",
    "- tokenization\n",
    "- lemmatization\n",
    "- sentence splitting\n",
    "- entity recognition\n",
    "- token vector representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "aWFBbIPFmSy4",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c063a433e96171b19c013141d9aed53",
     "grade": false,
     "grade_id": "cell-bdd9ef5d3e965340",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "df3375b7-c192-4759-d72e-e2ef23fde450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 s, sys: 551 ms, total: 14 s\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "P6Tqj95-mSy6",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dde683a4f8f25f1cf4e3cb04bd092989",
     "grade": false,
     "grade_id": "cell-11e9ffbf2f50c6eb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "text = \"This is the first sentence in this test string. The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "parsed_text = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is the first sentence in this test string. The quick brown fox jumps over the lazy dog.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_text.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "8Y9IDOFimSy8",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11bf7045ed3934b887d3f3c1edd270bf",
     "grade": false,
     "grade_id": "cell-d4194dd187aa95b1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "20fb432b-16ec-40e4-ba58-89c915caa9b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sentence: This is the first sentence in this test string.\n",
      "Lemmatization: this be the first sentence in this test string .\n",
      "Analyzing token: This\n",
      "This token is the first one in the sentence\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: this\n",
      "----------\n",
      "Analyzing token: is\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: AUX\n",
      "Lemma: be\n",
      "----------\n",
      "Analyzing token: the\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: the\n",
      "----------\n",
      "Analyzing token: first\n",
      "Stop word\n",
      "Entity type: ORDINAL\n",
      "Part of speech: ADJ\n",
      "Lemma: first\n",
      "----------\n",
      "Analyzing token: sentence\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: sentence\n",
      "----------\n",
      "Analyzing token: in\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: in\n",
      "----------\n",
      "Analyzing token: this\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: this\n",
      "----------\n",
      "Analyzing token: test\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: test\n",
      "----------\n",
      "Analyzing token: string\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: string\n",
      "----------\n",
      "Analyzing token: .\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: .\n",
      "----------\n",
      "--------------------------------------------------\n",
      "Analyzing sentence: The quick brown fox jumps over the lazy dog.\n",
      "Lemmatization: the quick brown fox jump over the lazy dog .\n",
      "Analyzing token: The\n",
      "This token is the first one in the sentence\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: the\n",
      "----------\n",
      "Analyzing token: quick\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: ADJ\n",
      "Lemma: quick\n",
      "----------\n",
      "Analyzing token: brown\n",
      "Not stop word\n",
      "Entity type: PERSON\n",
      "Part of speech: ADJ\n",
      "Lemma: brown\n",
      "----------\n",
      "Analyzing token: fox\n",
      "Not stop word\n",
      "Entity type: PERSON\n",
      "Part of speech: NOUN\n",
      "Lemma: fox\n",
      "----------\n",
      "Analyzing token: jumps\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: jump\n",
      "----------\n",
      "Analyzing token: over\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: over\n",
      "----------\n",
      "Analyzing token: the\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: the\n",
      "----------\n",
      "Analyzing token: lazy\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: ADJ\n",
      "Lemma: lazy\n",
      "----------\n",
      "Analyzing token: dog\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: dog\n",
      "----------\n",
      "Analyzing token: .\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: .\n",
      "----------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sent in parsed_text.sents:\n",
    "    print(f\"Analyzing sentence: {sent}\")\n",
    "    print(f\"Lemmatization: {sent.lemma_}\")\n",
    "    for token in sent:\n",
    "        print(f\"Analyzing token: {token}\")\n",
    "        if token.is_sent_start:\n",
    "            print(\"This token is the first one in the sentence\")\n",
    "        if token.is_stop:\n",
    "            print(\"Stop word\")\n",
    "        else:\n",
    "            print(\"Not stop word\")\n",
    "        print(f\"Entity type: {token.ent_type_}\")\n",
    "        print(f\"Part of speech: {token.pos_}\")\n",
    "        print(f\"Lemma: {token.lemma_}\")\n",
    "        print(\"-\"*10)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "ao9ZW4ySmSy_",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "778289b88c9003cc27f7839543106eb6",
     "grade": false,
     "grade_id": "cell-a1f85ff1f2b6b116",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### Come up with a couple sentences to test out and set the text to my_text\n",
    "### You can go to your favorite website or news source and copy a paragraph from there\n",
    "\n",
    "# YOUR CODE HERE\n",
    "my_text=\"It took me a long time and most of the world to learn what I know about love and fate and the choices we make, but the heart of it came to me in an instant, while I was chained to a wall and being tortured. I realised, somehow, through the screaming of my mind, that even in that shackled, bloody helplessness, I was still free: free to hate the men who were torturing me, or to forgive them. It doesnâ€™t sound like much, I know. But in the flinch and bite of the chain, when itâ€™s all youâ€™ve got, that freedom is an universe of possibility. And the choice you make between hating and forgiving, can become the story of your life.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = len(my_text.split())\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "J78DAPTnmSzC",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aadee940b59935acd80563ce3c6b248e",
     "grade": true,
     "grade_id": "cell-dd4c284f3101e2e6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(my_text) > 10\n",
    "assert my_text.count(\".\") > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(300,)\n",
      "1\n",
      "(300,)\n",
      "2\n",
      "(300,)\n",
      "3\n",
      "(300,)\n",
      "4\n",
      "(300,)\n",
      "5\n",
      "(300,)\n",
      "6\n",
      "(300,)\n",
      "7\n",
      "(300,)\n",
      "8\n",
      "(300,)\n",
      "9\n",
      "(300,)\n",
      "10\n",
      "(300,)\n",
      "11\n",
      "(300,)\n",
      "12\n",
      "(300,)\n",
      "13\n",
      "(300,)\n",
      "14\n",
      "(300,)\n",
      "15\n",
      "(300,)\n",
      "16\n",
      "(300,)\n",
      "17\n",
      "(300,)\n",
      "18\n",
      "(300,)\n",
      "19\n",
      "(300,)\n",
      "20\n",
      "(300,)\n",
      "21\n",
      "(300,)\n",
      "22\n",
      "(300,)\n",
      "23\n",
      "(300,)\n",
      "24\n",
      "(300,)\n",
      "25\n",
      "(300,)\n",
      "26\n",
      "(300,)\n",
      "27\n",
      "(300,)\n",
      "28\n",
      "(300,)\n",
      "29\n",
      "(300,)\n",
      "30\n",
      "(300,)\n",
      "31\n",
      "(300,)\n",
      "32\n",
      "(300,)\n",
      "33\n",
      "(300,)\n",
      "34\n",
      "(300,)\n",
      "35\n",
      "(300,)\n",
      "36\n",
      "(300,)\n",
      "37\n",
      "(300,)\n",
      "38\n",
      "(300,)\n",
      "39\n",
      "(300,)\n",
      "40\n",
      "(300,)\n",
      "41\n",
      "(300,)\n",
      "42\n",
      "(300,)\n",
      "43\n",
      "(300,)\n",
      "44\n",
      "(300,)\n",
      "45\n",
      "(300,)\n",
      "46\n",
      "(300,)\n",
      "47\n",
      "(300,)\n",
      "48\n",
      "(300,)\n",
      "49\n",
      "(300,)\n",
      "50\n",
      "(300,)\n",
      "51\n",
      "(300,)\n",
      "52\n",
      "(300,)\n",
      "53\n",
      "(300,)\n",
      "54\n",
      "(300,)\n",
      "55\n",
      "(300,)\n",
      "56\n",
      "(300,)\n",
      "57\n",
      "(300,)\n",
      "58\n",
      "(300,)\n",
      "59\n",
      "(300,)\n",
      "60\n",
      "(300,)\n",
      "61\n",
      "(300,)\n",
      "62\n",
      "(300,)\n",
      "63\n",
      "(300,)\n",
      "64\n",
      "(300,)\n",
      "65\n",
      "(300,)\n",
      "66\n",
      "(300,)\n",
      "67\n",
      "(300,)\n",
      "68\n",
      "(300,)\n",
      "69\n",
      "(300,)\n",
      "70\n",
      "(300,)\n",
      "71\n",
      "(300,)\n",
      "72\n",
      "(300,)\n",
      "73\n",
      "(300,)\n",
      "74\n",
      "(300,)\n",
      "75\n",
      "(300,)\n",
      "76\n",
      "(300,)\n",
      "77\n",
      "(300,)\n",
      "78\n",
      "(300,)\n",
      "79\n",
      "(300,)\n",
      "80\n",
      "(300,)\n",
      "81\n",
      "(300,)\n",
      "82\n",
      "(300,)\n",
      "83\n",
      "(300,)\n",
      "84\n",
      "(300,)\n",
      "85\n",
      "(300,)\n",
      "86\n",
      "(300,)\n",
      "87\n",
      "(300,)\n",
      "88\n",
      "(300,)\n",
      "89\n",
      "(300,)\n",
      "90\n",
      "(300,)\n",
      "91\n",
      "(300,)\n",
      "92\n",
      "(300,)\n",
      "93\n",
      "(300,)\n",
      "94\n",
      "(300,)\n",
      "95\n",
      "(300,)\n",
      "96\n",
      "(300,)\n",
      "97\n",
      "(300,)\n",
      "98\n",
      "(300,)\n",
      "99\n",
      "(300,)\n",
      "100\n",
      "(300,)\n",
      "101\n",
      "(300,)\n",
      "102\n",
      "(300,)\n",
      "103\n",
      "(300,)\n",
      "104\n",
      "(300,)\n",
      "105\n",
      "(300,)\n",
      "106\n",
      "(300,)\n",
      "107\n",
      "(300,)\n",
      "108\n",
      "(300,)\n",
      "109\n",
      "(300,)\n",
      "110\n",
      "(300,)\n",
      "111\n",
      "(300,)\n",
      "112\n",
      "(300,)\n",
      "113\n",
      "(300,)\n",
      "114\n",
      "(300,)\n",
      "115\n",
      "(300,)\n",
      "116\n",
      "(300,)\n",
      "117\n",
      "(300,)\n",
      "118\n",
      "(300,)\n",
      "119\n",
      "(300,)\n",
      "120\n",
      "(300,)\n",
      "121\n",
      "(300,)\n",
      "122\n",
      "(300,)\n",
      "123\n",
      "(300,)\n",
      "124\n",
      "(300,)\n",
      "125\n",
      "(300,)\n",
      "126\n",
      "(300,)\n",
      "127\n",
      "(300,)\n",
      "128\n",
      "(300,)\n",
      "129\n",
      "(300,)\n",
      "130\n",
      "(300,)\n",
      "131\n",
      "(300,)\n",
      "132\n",
      "(300,)\n",
      "133\n",
      "(300,)\n",
      "134\n",
      "(300,)\n",
      "135\n",
      "(300,)\n",
      "136\n",
      "(300,)\n",
      "137\n",
      "(300,)\n",
      "138\n",
      "(300,)\n",
      "139\n",
      "(300,)\n",
      "140\n",
      "(300,)\n",
      "141\n",
      "(300,)\n",
      "142\n",
      "(300,)\n",
      "143\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "parsed = nlp(my_text)\n",
    "i=0\n",
    "for token in parsed:\n",
    "    print(i)\n",
    "    print(token.vector.shape)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "Y6ANaZzvmSzF",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6193aa815fad57f5db9423bdb6d8b34",
     "grade": false,
     "grade_id": "cell-fd7fed9c7e3903f4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "dd6bc09b-8ab6-4eca-d209-acfd2fefddbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sentence: It took me a long time and most of the world to learn what I know about love and fate and the choices we make, but the heart of it came to me in an instant, while I was chained to a wall and being tortured.\n",
      "Lemmatization: -PRON- take -PRON- a long time and most of the world to learn what -PRON- know about love and fate and the choice -PRON- make , but the heart of -PRON- come to -PRON- in an instant , while -PRON- be chain to a wall and be torture .\n",
      "Analyzing token: It\n",
      "This token is the first one in the sentence\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: took\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: take\n",
      "----------\n",
      "Analyzing token: me\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: a\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: a\n",
      "----------\n",
      "Analyzing token: long\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: ADJ\n",
      "Lemma: long\n",
      "----------\n",
      "Analyzing token: time\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: time\n",
      "----------\n",
      "Analyzing token: and\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: CCONJ\n",
      "Lemma: and\n",
      "----------\n",
      "Analyzing token: most\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADJ\n",
      "Lemma: most\n",
      "----------\n",
      "Analyzing token: of\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: of\n",
      "----------\n",
      "Analyzing token: the\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: the\n",
      "----------\n",
      "Analyzing token: world\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: world\n",
      "----------\n",
      "Analyzing token: to\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PART\n",
      "Lemma: to\n",
      "----------\n",
      "Analyzing token: learn\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: learn\n",
      "----------\n",
      "Analyzing token: what\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: what\n",
      "----------\n",
      "Analyzing token: I\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: know\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: know\n",
      "----------\n",
      "Analyzing token: about\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: about\n",
      "----------\n",
      "Analyzing token: love\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: love\n",
      "----------\n",
      "Analyzing token: and\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: CCONJ\n",
      "Lemma: and\n",
      "----------\n",
      "Analyzing token: fate\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: fate\n",
      "----------\n",
      "Analyzing token: and\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: CCONJ\n",
      "Lemma: and\n",
      "----------\n",
      "Analyzing token: the\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: the\n",
      "----------\n",
      "Analyzing token: choices\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: choice\n",
      "----------\n",
      "Analyzing token: we\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: make\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: make\n",
      "----------\n",
      "Analyzing token: ,\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: ,\n",
      "----------\n",
      "Analyzing token: but\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: CCONJ\n",
      "Lemma: but\n",
      "----------\n",
      "Analyzing token: the\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: the\n",
      "----------\n",
      "Analyzing token: heart\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: heart\n",
      "----------\n",
      "Analyzing token: of\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: of\n",
      "----------\n",
      "Analyzing token: it\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: came\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: come\n",
      "----------\n",
      "Analyzing token: to\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: to\n",
      "----------\n",
      "Analyzing token: me\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: in\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: in\n",
      "----------\n",
      "Analyzing token: an\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: an\n",
      "----------\n",
      "Analyzing token: instant\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: instant\n",
      "----------\n",
      "Analyzing token: ,\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: ,\n",
      "----------\n",
      "Analyzing token: while\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: SCONJ\n",
      "Lemma: while\n",
      "----------\n",
      "Analyzing token: I\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: was\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: AUX\n",
      "Lemma: be\n",
      "----------\n",
      "Analyzing token: chained\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: chain\n",
      "----------\n",
      "Analyzing token: to\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: to\n",
      "----------\n",
      "Analyzing token: a\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: a\n",
      "----------\n",
      "Analyzing token: wall\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: wall\n",
      "----------\n",
      "Analyzing token: and\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: CCONJ\n",
      "Lemma: and\n",
      "----------\n",
      "Analyzing token: being\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: AUX\n",
      "Lemma: be\n",
      "----------\n",
      "Analyzing token: tortured\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: torture\n",
      "----------\n",
      "Analyzing token: .\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: .\n",
      "----------\n",
      "--------------------------------------------------\n",
      "Analyzing sentence: I realised, somehow, through the screaming of my mind, that even in that shackled, bloody helplessness, I was still free: free to hate the men who were torturing me, or to forgive them.\n",
      "Lemmatization: -PRON- realise , somehow , through the screaming of -PRON- mind , that even in that shackle , bloody helplessness , -PRON- be still free : free to hate the man who be torture -PRON- , or to forgive -PRON- .\n",
      "Analyzing token: I\n",
      "This token is the first one in the sentence\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: realised\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: realise\n",
      "----------\n",
      "Analyzing token: ,\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: ,\n",
      "----------\n",
      "Analyzing token: somehow\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADV\n",
      "Lemma: somehow\n",
      "----------\n",
      "Analyzing token: ,\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: ,\n",
      "----------\n",
      "Analyzing token: through\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: through\n",
      "----------\n",
      "Analyzing token: the\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: the\n",
      "----------\n",
      "Analyzing token: screaming\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: screaming\n",
      "----------\n",
      "Analyzing token: of\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: of\n",
      "----------\n",
      "Analyzing token: my\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: mind\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: mind\n",
      "----------\n",
      "Analyzing token: ,\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: ,\n",
      "----------\n",
      "Analyzing token: that\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: SCONJ\n",
      "Lemma: that\n",
      "----------\n",
      "Analyzing token: even\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADV\n",
      "Lemma: even\n",
      "----------\n",
      "Analyzing token: in\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: in\n",
      "----------\n",
      "Analyzing token: that\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: that\n",
      "----------\n",
      "Analyzing token: shackled\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: shackle\n",
      "----------\n",
      "Analyzing token: ,\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: ,\n",
      "----------\n",
      "Analyzing token: bloody\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: ADJ\n",
      "Lemma: bloody\n",
      "----------\n",
      "Analyzing token: helplessness\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: helplessness\n",
      "----------\n",
      "Analyzing token: ,\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: ,\n",
      "----------\n",
      "Analyzing token: I\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: was\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: AUX\n",
      "Lemma: be\n",
      "----------\n",
      "Analyzing token: still\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADV\n",
      "Lemma: still\n",
      "----------\n",
      "Analyzing token: free\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: ADJ\n",
      "Lemma: free\n",
      "----------\n",
      "Analyzing token: :\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: :\n",
      "----------\n",
      "Analyzing token: free\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: ADJ\n",
      "Lemma: free\n",
      "----------\n",
      "Analyzing token: to\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PART\n",
      "Lemma: to\n",
      "----------\n",
      "Analyzing token: hate\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: hate\n",
      "----------\n",
      "Analyzing token: the\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: the\n",
      "----------\n",
      "Analyzing token: men\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: man\n",
      "----------\n",
      "Analyzing token: who\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: who\n",
      "----------\n",
      "Analyzing token: were\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: AUX\n",
      "Lemma: be\n",
      "----------\n",
      "Analyzing token: torturing\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: torture\n",
      "----------\n",
      "Analyzing token: me\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: ,\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: ,\n",
      "----------\n",
      "Analyzing token: or\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: CCONJ\n",
      "Lemma: or\n",
      "----------\n",
      "Analyzing token: to\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PART\n",
      "Lemma: to\n",
      "----------\n",
      "Analyzing token: forgive\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: forgive\n",
      "----------\n",
      "Analyzing token: them\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: .\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: .\n",
      "----------\n",
      "--------------------------------------------------\n",
      "Analyzing sentence: It doesnâ€™t sound like much, I know.\n",
      "Lemmatization: -PRON- do not sound like much , -PRON- know .\n",
      "Analyzing token: It\n",
      "This token is the first one in the sentence\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: does\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: AUX\n",
      "Lemma: do\n",
      "----------\n",
      "Analyzing token: nâ€™t\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PART\n",
      "Lemma: not\n",
      "----------\n",
      "Analyzing token: sound\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: sound\n",
      "----------\n",
      "Analyzing token: like\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: SCONJ\n",
      "Lemma: like\n",
      "----------\n",
      "Analyzing token: much\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADJ\n",
      "Lemma: much\n",
      "----------\n",
      "Analyzing token: ,\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: ,\n",
      "----------\n",
      "Analyzing token: I\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: know\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: know\n",
      "----------\n",
      "Analyzing token: .\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: .\n",
      "----------\n",
      "--------------------------------------------------\n",
      "Analyzing sentence: But in the flinch and bite of the chain, when itâ€™s all youâ€™ve got, that freedom is an universe of possibility.\n",
      "Lemmatization: but in the flinch and bite of the chain , when -PRON- â€™ all -PRON- have get , that freedom be an universe of possibility .\n",
      "Analyzing token: But\n",
      "This token is the first one in the sentence\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: CCONJ\n",
      "Lemma: but\n",
      "----------\n",
      "Analyzing token: in\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: in\n",
      "----------\n",
      "Analyzing token: the\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: the\n",
      "----------\n",
      "Analyzing token: flinch\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: flinch\n",
      "----------\n",
      "Analyzing token: and\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: CCONJ\n",
      "Lemma: and\n",
      "----------\n",
      "Analyzing token: bite\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: bite\n",
      "----------\n",
      "Analyzing token: of\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: of\n",
      "----------\n",
      "Analyzing token: the\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: the\n",
      "----------\n",
      "Analyzing token: chain\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: chain\n",
      "----------\n",
      "Analyzing token: ,\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: ,\n",
      "----------\n",
      "Analyzing token: when\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADV\n",
      "Lemma: when\n",
      "----------\n",
      "Analyzing token: it\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: â€™s\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: â€™\n",
      "----------\n",
      "Analyzing token: all\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: all\n",
      "----------\n",
      "Analyzing token: you\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: â€™ve\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: have\n",
      "----------\n",
      "Analyzing token: got\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: get\n",
      "----------\n",
      "Analyzing token: ,\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: ,\n",
      "----------\n",
      "Analyzing token: that\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: that\n",
      "----------\n",
      "Analyzing token: freedom\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: freedom\n",
      "----------\n",
      "Analyzing token: is\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: AUX\n",
      "Lemma: be\n",
      "----------\n",
      "Analyzing token: an\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: an\n",
      "----------\n",
      "Analyzing token: universe\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: universe\n",
      "----------\n",
      "Analyzing token: of\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: of\n",
      "----------\n",
      "Analyzing token: possibility\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: possibility\n",
      "----------\n",
      "Analyzing token: .\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: .\n",
      "----------\n",
      "--------------------------------------------------\n",
      "Analyzing sentence: And the choice you make between hating and forgiving, can become the story of your life.\n",
      "Lemmatization: and the choice -PRON- make between hate and forgiving , can become the story of -PRON- life .\n",
      "Analyzing token: And\n",
      "This token is the first one in the sentence\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: CCONJ\n",
      "Lemma: and\n",
      "----------\n",
      "Analyzing token: the\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: the\n",
      "----------\n",
      "Analyzing token: choice\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: choice\n",
      "----------\n",
      "Analyzing token: you\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: PRON\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: make\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: make\n",
      "----------\n",
      "Analyzing token: between\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: between\n",
      "----------\n",
      "Analyzing token: hating\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: hate\n",
      "----------\n",
      "Analyzing token: and\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: CCONJ\n",
      "Lemma: and\n",
      "----------\n",
      "Analyzing token: forgiving\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: forgiving\n",
      "----------\n",
      "Analyzing token: ,\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: ,\n",
      "----------\n",
      "Analyzing token: can\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: can\n",
      "----------\n",
      "Analyzing token: become\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: VERB\n",
      "Lemma: become\n",
      "----------\n",
      "Analyzing token: the\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: the\n",
      "----------\n",
      "Analyzing token: story\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: story\n",
      "----------\n",
      "Analyzing token: of\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: ADP\n",
      "Lemma: of\n",
      "----------\n",
      "Analyzing token: your\n",
      "Stop word\n",
      "Entity type: \n",
      "Part of speech: DET\n",
      "Lemma: -PRON-\n",
      "----------\n",
      "Analyzing token: life\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: NOUN\n",
      "Lemma: life\n",
      "----------\n",
      "Analyzing token: .\n",
      "Not stop word\n",
      "Entity type: \n",
      "Part of speech: PUNCT\n",
      "Lemma: .\n",
      "----------\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "parsed = nlp(my_text)\n",
    "for sent in parsed.sents:\n",
    "    print(f\"Analyzing sentence: {sent}\")\n",
    "    print(f\"Lemmatization: {sent.lemma_}\")\n",
    "    for token in sent:\n",
    "        print(f\"Analyzing token: {token}\")\n",
    "        if token.is_sent_start:\n",
    "            print(\"This token is the first one in the sentence\")\n",
    "        if token.is_stop:\n",
    "            print(\"Stop word\")\n",
    "        else:\n",
    "            print(\"Not stop word\")\n",
    "        print(f\"Entity type: {token.ent_type_}\")\n",
    "        print(f\"Part of speech: {token.pos_}\")\n",
    "        print(f\"Lemma: {token.lemma_}\")\n",
    "        print(\"-\"*10)\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "U1SfqPUDmSzH",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a200a636496b7c4b0a02f61e4ac84a2f",
     "grade": false,
     "grade_id": "cell-1a2a413b9c440b95",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "If we use the larger spacy models, we get the GloVe representation for some words based on a pre-trained model. The GloVe vectors should be in 300 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "OdKLS41dmSzH",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9853f2d7bb9c9bac25000cc1c59d98c9",
     "grade": false,
     "grade_id": "cell-da272ac031f8d791",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "40e8ffe0-0217-4473-8a17-47b74228ae86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.012001   0.20751   -0.12578   -0.59325    0.12525    0.15975\n",
      "  0.13748   -0.33157   -0.13694    1.7893    -0.47094    0.70434\n",
      "  0.26673   -0.089961  -0.18168    0.067226   0.053347   1.5595\n",
      " -0.2541     0.038413  -0.01409    0.056774   0.023434   0.024042\n",
      "  0.31703    0.19025   -0.37505    0.035603   0.1181     0.012032\n",
      " -0.037566  -0.5046    -0.049261   0.092351   0.11031   -0.073062\n",
      "  0.33994    0.28239    0.13413    0.070128  -0.022099  -0.28103\n",
      "  0.49607   -0.48693   -0.090964  -0.1538    -0.38011   -0.014228\n",
      " -0.19392   -0.11068   -0.014088  -0.17906    0.24509   -0.16878\n",
      " -0.15351   -0.13808    0.02151    0.13699    0.0068061 -0.14915\n",
      " -0.38169    0.12727    0.44007    0.32678   -0.46117    0.068687\n",
      "  0.34747    0.18827   -0.31837    0.4447    -0.2095    -0.26987\n",
      "  0.48945    0.15388    0.05295   -0.049831   0.11207    0.14881\n",
      " -0.37003    0.30777   -0.33865    0.045149  -0.18987    0.26634\n",
      " -0.26401   -0.47556    0.68381   -0.30653    0.24606    0.31611\n",
      " -0.071098   0.030417   0.088119   0.045025   0.20125   -0.21618\n",
      " -0.36371   -0.25948   -0.42398   -0.14305   -0.10208    0.21498\n",
      " -0.21924   -0.17935    0.21546    0.13801    0.24504   -0.2559\n",
      "  0.054815   0.21307    0.2564    -0.25673    0.17961   -0.47638\n",
      " -0.25181   -0.0091498 -0.054362  -0.21007    0.12597   -0.40795\n",
      " -0.021164   0.20585    0.18925   -0.0051896 -0.51394    0.28862\n",
      " -0.077748  -0.27676    0.46567   -0.14225   -0.17879   -0.4357\n",
      " -0.32481    0.15034   -0.058367   0.49652    0.20472    0.019866\n",
      "  0.13326    0.12823   -1.0177     0.29007    0.28995    0.029994\n",
      " -0.10763    0.28665   -0.24387    0.22905   -0.26249   -0.069269\n",
      " -0.17889    0.21936    0.15146    0.04567   -0.050497   0.071482\n",
      " -0.1027    -0.080705   0.30296    0.031302   0.26613   -0.0060951\n",
      "  0.10313   -0.39987   -0.043945  -0.057625   0.08702   -0.098152\n",
      "  0.22835   -0.005211   0.038075   0.01591   -0.20622    0.021853\n",
      "  0.0040426 -0.043063  -0.002294  -0.26097   -0.25802   -0.28158\n",
      " -0.23118   -0.010404  -0.30102   -0.4042     0.014653  -0.10445\n",
      "  0.30377   -0.20957    0.3119     0.068272   0.1008     0.010423\n",
      "  0.54011    0.29865    0.12653    0.013761   0.21738   -0.39521\n",
      "  0.066633   0.50327    0.14913   -0.11554    0.010042   0.095698\n",
      "  0.16607   -0.18808    0.055019   0.026715  -0.3164    -0.046583\n",
      " -0.051591   0.023475  -0.11007    0.085642   0.28394    0.040497\n",
      "  0.071986   0.14157   -0.021199   0.44718    0.20088   -0.12964\n",
      " -0.067183   0.47614    0.13394   -0.17287   -0.37324   -0.17285\n",
      "  0.02683   -0.1316     0.09116   -0.46487    0.1274    -0.090159\n",
      " -0.10552    0.068006  -0.13381    0.17056    0.089509  -0.23133\n",
      " -0.27572    0.061534  -0.051646   0.28377    0.25286   -0.24139\n",
      " -0.19905    0.12049   -0.1011     0.27392    0.27843    0.26449\n",
      " -0.18292   -0.048961   0.19198    0.17192    0.33659   -0.20184\n",
      " -0.34305   -0.24553   -0.15399    0.3945     0.22839   -0.25753\n",
      " -0.25675   -0.37332   -0.23884   -0.048816   0.78323    0.18851\n",
      " -0.26477    0.096566   0.062658  -0.30668   -0.43334    0.10006\n",
      "  0.21136    0.039459  -0.11077    0.24421    0.60942   -0.46646\n",
      "  0.086385  -0.39702   -0.23363    0.021307  -0.10778   -0.2281\n",
      "  0.50803    0.11567    0.16165   -0.066737  -0.29556    0.022612\n",
      " -0.28135    0.0635     0.14019    0.13871   -0.36049   -0.035    ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(token.vector)\n",
    "token.vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "udWQK0VemSzJ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47b0eee7c6be8d8bbc285da281d4f0d8",
     "grade": false,
     "grade_id": "cell-a189f31d66863678",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "One thing that spaCy allows us to easily do with their vectors is identify the similarity between one word and another. Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fFn5sVhmSzJ"
   },
   "outputs": [],
   "source": [
    "# Try out different words here and see if the results you get seem reasonable.\n",
    "first_word = \"cat\" \n",
    "second_word = \"dishwasher\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "h7bO-8i5mSzL",
    "outputId": "31bea4ff-76ff-46b0-da47-a40ca6834ef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1805633495837248\n"
     ]
    }
   ],
   "source": [
    "print(nlp(first_word).similarity(nlp(second_word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "RijrqcAymSzM",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c68c644df1851056a4f8f93cda99494",
     "grade": false,
     "grade_id": "cell-7d1247a2cf60bab3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Given that the parsing of text takes some time, we will only consider the first 1000 articles in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "Ab2iLPprmSzN",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d075c5070bd31c803ad35d1e57b88ee",
     "grade": false,
     "grade_id": "cell-65e1d1dbb5567b87",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "new_X_train, new_X_test, new_y_train, new_y_test = train_test_split(X_train[:1000], y_train[:1000], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train, new_X_test, new_y_train, new_y_test = train_test_split(X_train[:2], y_train[:2], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n First of all I think when ESPN covers the game they do a wonderful job, but\\n last night I felt the same way. I really hate watching Devils/Pens game. \\n Everyone knows that Devils are going to get their ass kicked, why even \\n bother showing them. I was so bored and these ESPN people don't seem to have\\n any brain. After the Sundays and last night games, they are still going to \\n show Devils/Pens on THU and SUN. WHat the hell are they thinking about? I \\n think if they keep it up like this NHL will never get a major network contract.\\n I'd rather see Caps/Isls game which is more exciting. \\n\\n Now I just hope all baseball games are rained out on ESPN so atleast\\n we can get diffrent hockey games. \\n\\n  \\n\\n\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_train[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lYfzjgximSzO"
   },
   "source": [
    "This [tweet](https://twitter.com/_inesmontani/status/1113413036985991170) may be relevant to understanding the most performant ways to use spacy in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "deletable": false,
    "id": "ALRz2CBOmSzP",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79e38d9aca3fab8a8c6796a821c722a6",
     "grade": false,
     "grade_id": "cell-a35e8a4ea5c845f6",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "b733a3f5-433a-4354-ff6b-f94a71ac925e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.6 s, sys: 1.64 s, total: 25.2 s\n",
      "Wall time: 25.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Using `nlp` from above, parse every instance of new_X_train\n",
    "# save the document **vectors** to a np.array called X_train_glove\n",
    "# This cell will take a long time to run. \n",
    "# Try changing the number of articles to run this on to just 10 until you pass the asserts\n",
    "# Then change it back to 1000\n",
    "\n",
    "# YOUR CODE HERE\n",
    "X_train_glove= np.array([nlp(text).vector for text in new_X_train])\n",
    "X_test_glove= np.array([nlp(text).vector for text in new_X_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=nlp(new_X_train[0])\n",
    "a.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(750, 300)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "aE4jmUhdmSzQ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3cbbe22214eea789813b27bee43dc314",
     "grade": true,
     "grade_id": "cell-8431bb15ebe0914f",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert X_train_glove.shape == (len(new_X_train), 300)\n",
    "assert X_test_glove.shape == (len(new_X_test), 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "pjlHW5cjmSzS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d13cff29a75bb3f0b8087af24238c43",
     "grade": false,
     "grade_id": "cell-5b693b9255ee6aea",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "81f3ff2c-355e-4c50-cf5f-c8ced389df83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         7\n",
      "           1       0.62      0.50      0.56        10\n",
      "           2       0.38      0.45      0.42        11\n",
      "           3       0.50      0.58      0.54        12\n",
      "           4       0.43      0.50      0.46        12\n",
      "           5       0.70      0.64      0.67        11\n",
      "           6       0.46      0.60      0.52        10\n",
      "           7       0.78      0.78      0.78        18\n",
      "           8       0.67      0.71      0.69        17\n",
      "           9       0.63      0.71      0.67        17\n",
      "          10       0.62      0.71      0.67        14\n",
      "          11       0.70      0.50      0.58        14\n",
      "          12       0.58      0.37      0.45        19\n",
      "          13       0.75      0.94      0.83        16\n",
      "          14       0.54      0.70      0.61        10\n",
      "          15       0.44      0.62      0.52        13\n",
      "          16       0.78      0.50      0.61        14\n",
      "          17       0.50      0.67      0.57         9\n",
      "          18       0.50      0.27      0.35        11\n",
      "          19       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.58       250\n",
      "   macro avg       0.53      0.54      0.52       250\n",
      "weighted avg       0.57      0.58      0.57       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC().fit(X_train_glove, new_y_train)\n",
    "y_pred = svm.predict(X_test_glove)\n",
    "print(classification_report(new_y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pzA6ypN_mSzT"
   },
   "source": [
    "Note that the results here aren't necessarily a fair comparison since we are only using a small subset of the data for both training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "673h2TJlmSzU",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e662876feeb742e62383826b6fcb418",
     "grade": false,
     "grade_id": "cell-cbdc66865e98beb5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "We will not cover LDA in this exercise but if you are interested in topic modeling, you should check out [Gensim](https://radimrehurek.com/gensim/) and its [LDA implementation](https://radimrehurek.com/gensim/models/ldamodel.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "vK5WPN-XmSzV",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "483e65acc4062f57f7320d9b4cb0f945",
     "grade": false,
     "grade_id": "cell-75181722913aa756",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "vEIhPAnWmSzW",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed936ab53a1391c5e6af8df699a1dbf5",
     "grade": false,
     "grade_id": "feedback",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def feedback():\n",
    "    \"\"\"Provide feedback on the contents of this exercise\n",
    "    \n",
    "    Returns:\n",
    "        string\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    return \"I didn't know anything about this but now I am interested. thanks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "Tk6E2rzLmSzY",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f39f6185a54850c2f1f9b5b2a17b7543",
     "grade": true,
     "grade_id": "feedback-tests",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "18 - Natural Language Processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
